{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xiaoyi Wang\n",
    "\n",
    "2403234885"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Tweet analysis - NER and sentiment extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is a collection of tweets and is split into 2 files - `train.csv` and `test.csv`. Do not use `test.csv` during any stage of the training process. The csv files contain the following columns:\n",
    "- `textID` - unique ID for each piece of text\n",
    "- `text` - the text of the tweet\n",
    "- `sentiment` - the sentiment of the tweet (positive, negative, neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import necessary libraries, and load both the train and test datasets into pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3539</th>\n",
       "      <td>e9a87971b0</td>\n",
       "      <td>i need some hot green tea  I cant sleep  ....</td>\n",
       "      <td>cant sleep  ....</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8916</th>\n",
       "      <td>bff0c7715b</td>\n",
       "      <td>Finished Death Du Jour. School`s out.  Nerd.</td>\n",
       "      <td>Finished Death Du Jour. School`s out.  Nerd.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6448</th>\n",
       "      <td>3bade8ec46</td>\n",
       "      <td>Good for you! I don`t think I`ll be joining l...</td>\n",
       "      <td>WHEE!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>2a7f29502f</td>\n",
       "      <td>Florida should be nice.</td>\n",
       "      <td>nice.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>1943a08c94</td>\n",
       "      <td>I forgot about it and I already ate lunch  so...</td>\n",
       "      <td>I forgot about it and I already ate lunch  so ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                               text  \\\n",
       "3539  e9a87971b0      i need some hot green tea  I cant sleep  ....   \n",
       "8916  bff0c7715b       Finished Death Du Jour. School`s out.  Nerd.   \n",
       "6448  3bade8ec46   Good for you! I don`t think I`ll be joining l...   \n",
       "4022  2a7f29502f                            Florida should be nice.   \n",
       "2207  1943a08c94   I forgot about it and I already ate lunch  so...   \n",
       "\n",
       "                                          selected_text sentiment  \n",
       "3539                                   cant sleep  ....  negative  \n",
       "8916       Finished Death Du Jour. School`s out.  Nerd.   neutral  \n",
       "6448                                              WHEE!  positive  \n",
       "4022                                              nice.  positive  \n",
       "2207  I forgot about it and I already ate lunch  so ...   neutral  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6878</th>\n",
       "      <td>3d8e8979e4</td>\n",
       "      <td>http://bit.ly/253ce  :: I got bit in the face ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>3c8e8d1940</td>\n",
       "      <td>my son got stung by a bug for the first time  ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17101</th>\n",
       "      <td>04100a8743</td>\n",
       "      <td>Morning.  Tweet Tweet.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10826</th>\n",
       "      <td>e1a023ed94</td>\n",
       "      <td>not good you`re not comin close to where i lo...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25061</th>\n",
       "      <td>99ee365e3b</td>\n",
       "      <td>Apple is also rotten in the center, just my luck</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text sentiment\n",
       "6878   3d8e8979e4  http://bit.ly/253ce  :: I got bit in the face ...   neutral\n",
       "477    3c8e8d1940  my son got stung by a bug for the first time  ...  negative\n",
       "17101  04100a8743                             Morning.  Tweet Tweet.   neutral\n",
       "10826  e1a023ed94   not good you`re not comin close to where i lo...   neutral\n",
       "25061  99ee365e3b   Apple is also rotten in the center, just my luck  negative"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop('selected_text', axis=1)\n",
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3250</th>\n",
       "      <td>1012d3a39c</td>\n",
       "      <td>Oh dear... that`s just... disturbing.   The c...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3284</th>\n",
       "      <td>864cddfae8</td>\n",
       "      <td>ooo my goddddd; vodafone trouble. no network</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>8f3fc77f66</td>\n",
       "      <td>- brief interruption- brb. goodnight if i ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3306</th>\n",
       "      <td>6ff929e04b</td>\n",
       "      <td>_xo Today yeah</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>af63915fa9</td>\n",
       "      <td>I`m leaving work now. Trying to decide if I re...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                               text sentiment\n",
       "3250  1012d3a39c   Oh dear... that`s just... disturbing.   The c...  negative\n",
       "3284  864cddfae8       ooo my goddddd; vodafone trouble. no network  negative\n",
       "1712  8f3fc77f66      - brief interruption- brb. goodnight if i ...  positive\n",
       "3306  6ff929e04b                                     _xo Today yeah   neutral\n",
       "98    af63915fa9  I`m leaving work now. Trying to decide if I re...   neutral"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Check for any missing values in the train and test datasets. If there are any, drop the rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textID       0\n",
       "text         1\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textID       0\n",
       "text         0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the row with missing values in train\n",
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textID       0\n",
       "text         0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Check the distributions of the `sentiment` in the train and test datasets. Comment if there are any differences and whether this is a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     11117\n",
       "positive     8582\n",
       "negative     7781\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     0.404549\n",
       "positive    0.312300\n",
       "negative    0.283151\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['sentiment'].value_counts()/len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "train['sentiment'].value_counts().plot(kind='bar', ax=ax, label='Train Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     1430\n",
       "positive    1103\n",
       "negative    1001\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     0.404641\n",
       "positive    0.312111\n",
       "negative    0.283248\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['sentiment'].value_counts()/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "test['sentiment'].value_counts().plot(kind='bar', ax=ax, label='Test Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no significant difference in the sentiment distributions between the train and test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Split the training dataset into train and validation sets. The validation set will be used to evaluate the model performance, tune hyperparameters, and make improvements during training. The test set will only be used to evaluate the final model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train['text']\n",
    "y = train['sentiment']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Use TF-IDF to vectorize the tweets. Write a function to take in various classifiers (`SGDClassifier`, `RidgeClassifier`, `LinearSVC`, etc.) and train them on the training set. The function should return the predictions so that it can be evaluated on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def train_classifier(X_train, X_val, y_train, classifier):\n",
    "    # Initialize the TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Perform TF-IDF vectorization on the training and validation text data\n",
    "    train_features = vectorizer.fit_transform(X_train)\n",
    "    validation_features = vectorizer.transform(X_val)\n",
    "\n",
    "    # Train the classifier on the training features and labels\n",
    "    classifier.fit(train_features, y_train)\n",
    "        \n",
    "    # Make predictions on the validation set\n",
    "    pred = classifier.predict(validation_features)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Now run the function on the validation set and print the `classification_report` for `SGDClassifier`, `RidgeClassifier` and `LinearSVC`. Which classifier performs the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Get the predictions on various classifiers\n",
    "pred_sgdc = train_classifier(X_train, X_val, y_train, SGDClassifier())\n",
    "pred_ridge = train_classifier(X_train, X_val, y_train, RidgeClassifier())\n",
    "pred_svc= train_classifier(X_train, X_val, y_train, LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.58      0.64      1572\n",
      "     neutral       0.64      0.75      0.69      2236\n",
      "    positive       0.78      0.74      0.76      1688\n",
      "\n",
      "    accuracy                           0.70      5496\n",
      "   macro avg       0.71      0.69      0.70      5496\n",
      "weighted avg       0.70      0.70      0.70      5496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_val,pred_sgdc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.60      0.64      1572\n",
      "     neutral       0.63      0.70      0.66      2236\n",
      "    positive       0.75      0.72      0.73      1688\n",
      "\n",
      "    accuracy                           0.68      5496\n",
      "   macro avg       0.69      0.67      0.68      5496\n",
      "weighted avg       0.68      0.68      0.68      5496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_val,pred_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.62      0.65      1572\n",
      "     neutral       0.63      0.67      0.65      2236\n",
      "    positive       0.74      0.73      0.73      1688\n",
      "\n",
      "    accuracy                           0.67      5496\n",
      "   macro avg       0.68      0.67      0.68      5496\n",
      "weighted avg       0.68      0.67      0.67      5496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_val,pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGDClassifier gives the overall best performance in terms of accuracy, precision, recall, and f1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Finally, print the `classification_report` for the test set using the best classifier from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test['text']\n",
    "y_test = test['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_best = train_classifier(X_train, X_test, y_train, SGDClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.62      0.67      1001\n",
      "     neutral       0.64      0.75      0.69      1430\n",
      "    positive       0.79      0.72      0.76      1103\n",
      "\n",
      "    accuracy                           0.71      3534\n",
      "   macro avg       0.72      0.70      0.71      3534\n",
      "weighted avg       0.71      0.71      0.71      3534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the `en_core_web_lg` model to extract named entities from the tweets. Write a function that takes in a tweet and returns a list of named entities. Note that some tweets may not contain any named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# ! python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `spacy.displacy.render(doc, style='ent',jupyter=True)` which can be used to visualize entities with colors and labels in a Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def extract_named_entities(tweet):\n",
    "    # Load the 'en_core_web_lg' model\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    \n",
    "    # Process the tweet using the model\n",
    "    doc = nlp(tweet)\n",
    "    \n",
    "    # Extract the named entities from the processed tweet\n",
    "    named_entities = [ent.text for ent in doc.ents]\n",
    "    \n",
    "    # Visualize entities\n",
    "    return spacy.displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I really really like the song \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Love Story\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       " by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Taylor Swift\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extract_named_entities(train['text'][11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For the positively and negatively classified tweets we will now extract the named entities and print the top 5 most common entities for each sentiment. First, create a function to extract the top `n` entities given a dataframe and the sentiment of interest. Refer to the notebooks from previous sessions for examples on how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "def extract_top_entities(df, sentiment, n):\n",
    "    # Filter the DataFrame for the given sentiment\n",
    "    filtered_df = df[df['sentiment'] == sentiment]\n",
    "    \n",
    "    # Load the 'en_core_web_lg' model\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    \n",
    "    # Initialize entity_counts dictionary\n",
    "    entity_counts = {}\n",
    "    \n",
    "    # Iterate over each row in the filtered DataFrame\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        # Get the text of the current tweet\n",
    "        tweet_text = row['text']\n",
    "        \n",
    "        # Process the tweet text using the model\n",
    "        doc = nlp(tweet_text)\n",
    "        \n",
    "        # Count the occurrences of each named entity\n",
    "        for ent in doc.ents:\n",
    "            if ent.text not in entity_counts:\n",
    "                entity_counts[ent.text] = 1\n",
    "            else:\n",
    "                entity_counts[ent.text] += 1\n",
    "    \n",
    "    # Sort the entities based on their occurrence counts\n",
    "    sorted_entities = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Extract the top n entities\n",
    "    top_entities = [entity[0] for entity in sorted_entities[:n]]\n",
    "    \n",
    "    return top_entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now, use the function to extract the top 5 entities for positive and negative tweets. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today', 'tonight', 'tomorrow', '2', 'one']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_top_entities(train, 'positive', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today', 'tonight', '2', 'tomorrow', 'one']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_top_entities(train, 'negative', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results do not seem very useful because the top 5 entities in positive and negative tweets are the same. They do not differentiate between sentiments. We should look for more useful NER tags information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Note that some entities types may not contribute to the sentiment of the tweet. For example, the entity `ORG` may not be useful in determining the sentiment of the tweet. Use the `spacy.explain()` function to find out what each entity type means. Create a list of entity types that you think are not useful in determining the sentiment of the tweet. Modify the function from the previous step to exclude these entity types and print the top 5 entities for positive and negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to display basic entity info:\n",
    "def show_ents(doc):\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))\n",
    "    else:\n",
    "        print('No named entities found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the NERs that are in both pos and neg top entities and exclude them. Because they do not differentiate between sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tonight - TIME - Times smaller than a day\n"
     ]
    }
   ],
   "source": [
    "show_ents(nlp('tonight'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_entities_exclude(df, sentiment, n, exclude_types=[]):\n",
    "    # Filter the DataFrame for the given sentiment\n",
    "    filtered_df = df[df['sentiment'] == sentiment]\n",
    "    \n",
    "    # Load the 'en_core_web_lg' model\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    \n",
    "    # Initialize entity_counts dictionary\n",
    "    entity_counts = {}\n",
    "    \n",
    "    # Iterate over each row in the filtered DataFrame\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        # Get the text of the current tweet\n",
    "        tweet_text = row['text']\n",
    "        \n",
    "        # Process the tweet text using the model\n",
    "        doc = nlp(tweet_text)\n",
    "        \n",
    "        # Count the occurrences of each named entity, excluding the list\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ not in exclude_types:\n",
    "                if ent.text not in entity_counts:\n",
    "                    entity_counts[ent.text] = 1\n",
    "                else:\n",
    "                    entity_counts[ent.text] += 1\n",
    "    \n",
    "    # Sort the entities based on their occurrence counts\n",
    "    sorted_entities = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Extract the top n entities\n",
    "    top_entities = [entity[0] for entity in sorted_entities[:n]]\n",
    "    \n",
    "    return top_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_useful = ['ORG', 'DATE', 'TIME', 'CARDINAL', 'ORDINAL', 'NORP', 'GPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love - WORK_OF_ART - Titles of books, songs, etc.\n"
     ]
    }
   ],
   "source": [
    "show_ents(nlp('Love'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Goodnight', 'Love', 'YAY', 'hun', 'Mothers Day']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_top_entities_exclude(train, 'positive', 5, not_useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English', '100%', 'english', 'Boo', 'Sooo']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_top_entities_exclude(train, 'negative', 5, not_useful)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 2 different sets of top entities for positive vs. negative sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Text generation with neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Refer to the text generation notebook from session 7 for more details on how to do this.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select an arbitrary text of your choice for the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Open the PDF file in binary mode\n",
    "with open('sample_text.pdf', 'rb') as file:\n",
    "    # Create a PDF reader object\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "    # Loop through the first two pages and extract the text\n",
    "    for page in range(2):\n",
    "        # Get the text content of the current page\n",
    "        page_obj = pdf_reader.pages[page]\n",
    "        text = page_obj.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Through biology and biological scientists, we are able to identify the COVID – 19, immediately recognizing it as an environmental threat. All information about this new virus was all because of biology. Biological scientists are able to describe it and identify it by studying the virus’ dynamics, utilizing biological principles to understand the whole thing. For an instance, four biology professors of UC San Diego have gathered for a special roundtable analysis of the recently discovered coronavirus hosted by UCTV. They discussed the biological roots and evolution of COVID–19. Emily Troemel, a professor who studies host-pathogen interactions discussed the basic biological aspects of coronaviruses and described what scientists have learned and discovered so far about the new virus. She stated that the virus has RNA in its genome and it will help them understand how scientists and medical practitioners will test for the presence of coronavirus. They were also able to take a look at the changes in the sequence of the viral genome which is a big help in tracking the spread of the virus around the world. Matt Daugherty, another professor who studies the evolutionary arms race that pits the immune systems of hosts on one hand and pathogens on the other, even though COVID–19 has a zoonotic origin and is able to adapt to a range of genetic differences between the original host species and humans, effective vaccines can ultimately destroy it. Another professor from the Section of Ecology, Behaviour, and Evolution named Justin Meyer also discussed in the video concepts of how can science and society predict future pandemics. Overall, the content of the video showed us how biology and science can help us understand COVID–19 and its dynamics with Troemel even stating that “We can learn about how the biology of the virus is changing and how it may be altering the way it interacts with host cells, and also potentially different ways that we could treat it. It’s part of an amazing open science effort with an unprecedented level of information acquisition and information sharing among researchers.” Scientific statements, information, and judgments like these from those professionals and knowledgeable about the new virus are really helpful, especially to our government. These statements, information, and judgments that we have gathered from various professionals not only will help us have a deeper understanding of the new virus and what is happening around us but it will also become our basis on how we are going to respond and handle it. Things like this were also crucial in helping us find a probable or possible cure or solution that will end this pandemic and get back to our normal lives where we can enjoy the company of our family and friends once more. Conclusion Biology is not just one of those branches of science that deals with life and other living organisms, our teacher taught us these in school but they were actually a part of our daily lives and helped us understand our own bodies more to get through each day in our lives. Biology as well as other sciences and technology are helping us understand everything around us thus making us survive each day especially now that we are experiencing this global pandemic caused by the new virus called COVID–19. I hope everyone will understand and see the big difference that these various fields of science can make in order for us to get through this pandemic. Let us all support and salute our brave scientists, medical practitioners, and other frontliners who despite the risks of exposure to the virus still continue to do their best in finding the best possible care and probable cure that will end this pandemic. \n"
     ]
    }
   ],
   "source": [
    "# decode the string by interpreting any escape sequences it contains\n",
    "decoded_text = text.replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "clean_text = re.sub('[^a-zA-z]', ' ', decoded_text)\n",
    "clean_text = clean_text.lower()\n",
    "tokens = clean_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sequences of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize into sequences of tokens\n",
    "train_len = 25+1 # 25 training words, then one target word\n",
    "\n",
    "# Empty list of sequences\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len, len(tokens)):\n",
    "    \n",
    "    # Grab train_len# amount of characters\n",
    "    seq = tokens[i-train_len:i]\n",
    "    \n",
    "    # Add to list of sequences\n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'through biology and biological scientists we are able to identify the covid immediately recognizing it as an environmental threat all information about this new virus was'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install keras\n",
    "# ! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 : through\n",
      "14 : biology\n",
      "2 : and\n",
      "27 : biological\n",
      "26 : scientists\n",
      "12 : we\n",
      "18 : are\n",
      "36 : able\n",
      "5 : to\n",
      "86 : identify\n",
      "1 : the\n",
      "25 : covid\n",
      "279 : immediately\n",
      "278 : recognizing\n",
      "6 : it\n",
      "51 : as\n",
      "35 : an\n",
      "274 : environmental\n",
      "272 : threat\n",
      "50 : all\n",
      "24 : information\n",
      "34 : about\n",
      "22 : this\n",
      "23 : new\n",
      "7 : virus\n",
      "91 : was\n"
     ]
    }
   ],
   "source": [
    "for i in sequences[0]:\n",
    "    print(f'{i} : {tokenizer.index_word[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('through', 53),\n",
       "             ('biology', 158),\n",
       "             ('and', 764),\n",
       "             ('biological', 108),\n",
       "             ('scientists', 109),\n",
       "             ('we', 162),\n",
       "             ('are', 137),\n",
       "             ('able', 86),\n",
       "             ('to', 308),\n",
       "             ('identify', 36),\n",
       "             ('the', 849),\n",
       "             ('covid', 116),\n",
       "             ('immediately', 13),\n",
       "             ('recognizing', 14),\n",
       "             ('it', 275),\n",
       "             ('as', 68),\n",
       "             ('an', 95),\n",
       "             ('environmental', 18),\n",
       "             ('threat', 19),\n",
       "             ('all', 72),\n",
       "             ('information', 125),\n",
       "             ('about', 100),\n",
       "             ('this', 128),\n",
       "             ('new', 128),\n",
       "             ('virus', 253),\n",
       "             ('was', 26),\n",
       "             ('because', 26),\n",
       "             ('of', 596),\n",
       "             ('describe', 26),\n",
       "             ('by', 78),\n",
       "             ('studying', 26),\n",
       "             ('dynamics', 52),\n",
       "             ('utilizing', 26),\n",
       "             ('principles', 26),\n",
       "             ('understand', 156),\n",
       "             ('whole', 26),\n",
       "             ('thing', 26),\n",
       "             ('for', 104),\n",
       "             ('instance', 26),\n",
       "             ('four', 26),\n",
       "             ('professors', 26),\n",
       "             ('uc', 26),\n",
       "             ('san', 26),\n",
       "             ('diego', 26),\n",
       "             ('have', 104),\n",
       "             ('gathered', 52),\n",
       "             ('a', 234),\n",
       "             ('special', 26),\n",
       "             ('roundtable', 26),\n",
       "             ('analysis', 26),\n",
       "             ('recently', 26),\n",
       "             ('discovered', 52),\n",
       "             ('coronavirus', 52),\n",
       "             ('hosted', 26),\n",
       "             ('uctv', 26),\n",
       "             ('they', 78),\n",
       "             ('discussed', 78),\n",
       "             ('roots', 26),\n",
       "             ('evolution', 52),\n",
       "             ('emily', 26),\n",
       "             ('troemel', 52),\n",
       "             ('professor', 78),\n",
       "             ('who', 78),\n",
       "             ('studies', 52),\n",
       "             ('host', 78),\n",
       "             ('pathogen', 26),\n",
       "             ('interactions', 26),\n",
       "             ('basic', 26),\n",
       "             ('aspects', 26),\n",
       "             ('coronaviruses', 26),\n",
       "             ('described', 26),\n",
       "             ('what', 52),\n",
       "             ('learned', 26),\n",
       "             ('so', 26),\n",
       "             ('far', 26),\n",
       "             ('she', 26),\n",
       "             ('stated', 26),\n",
       "             ('that', 238),\n",
       "             ('has', 52),\n",
       "             ('rna', 26),\n",
       "             ('in', 221),\n",
       "             ('its', 52),\n",
       "             ('genome', 52),\n",
       "             ('will', 159),\n",
       "             ('help', 104),\n",
       "             ('them', 26),\n",
       "             ('how', 156),\n",
       "             ('medical', 52),\n",
       "             ('practitioners', 52),\n",
       "             ('test', 26),\n",
       "             ('presence', 26),\n",
       "             ('were', 78),\n",
       "             ('also', 130),\n",
       "             ('take', 26),\n",
       "             ('look', 26),\n",
       "             ('at', 26),\n",
       "             ('changes', 26),\n",
       "             ('sequence', 26),\n",
       "             ('viral', 26),\n",
       "             ('which', 26),\n",
       "             ('is', 130),\n",
       "             ('big', 52),\n",
       "             ('tracking', 26),\n",
       "             ('spread', 26),\n",
       "             ('around', 78),\n",
       "             ('world', 26),\n",
       "             ('matt', 26),\n",
       "             ('daugherty', 26),\n",
       "             ('another', 52),\n",
       "             ('evolutionary', 26),\n",
       "             ('arms', 26),\n",
       "             ('race', 26),\n",
       "             ('pits', 26),\n",
       "             ('immune', 26),\n",
       "             ('systems', 26),\n",
       "             ('hosts', 26),\n",
       "             ('on', 78),\n",
       "             ('one', 52),\n",
       "             ('hand', 26),\n",
       "             ('pathogens', 26),\n",
       "             ('other', 104),\n",
       "             ('even', 52),\n",
       "             ('though', 26),\n",
       "             ('zoonotic', 26),\n",
       "             ('origin', 26),\n",
       "             ('adapt', 26),\n",
       "             ('range', 26),\n",
       "             ('genetic', 26),\n",
       "             ('differences', 26),\n",
       "             ('between', 26),\n",
       "             ('original', 26),\n",
       "             ('species', 26),\n",
       "             ('humans', 26),\n",
       "             ('effective', 26),\n",
       "             ('vaccines', 26),\n",
       "             ('can', 156),\n",
       "             ('ultimately', 26),\n",
       "             ('destroy', 26),\n",
       "             ('from', 78),\n",
       "             ('section', 26),\n",
       "             ('ecology', 26),\n",
       "             ('behaviour', 26),\n",
       "             ('named', 26),\n",
       "             ('justin', 26),\n",
       "             ('meyer', 26),\n",
       "             ('video', 52),\n",
       "             ('concepts', 26),\n",
       "             ('science', 130),\n",
       "             ('society', 26),\n",
       "             ('predict', 26),\n",
       "             ('future', 26),\n",
       "             ('pandemics', 26),\n",
       "             ('overall', 26),\n",
       "             ('content', 26),\n",
       "             ('showed', 26),\n",
       "             ('us', 312),\n",
       "             ('with', 104),\n",
       "             ('stating', 26),\n",
       "             ('learn', 26),\n",
       "             ('changing', 26),\n",
       "             ('may', 26),\n",
       "             ('be', 26),\n",
       "             ('altering', 26),\n",
       "             ('way', 26),\n",
       "             ('interacts', 26),\n",
       "             ('cells', 26),\n",
       "             ('potentially', 26),\n",
       "             ('different', 26),\n",
       "             ('ways', 26),\n",
       "             ('could', 26),\n",
       "             ('treat', 26),\n",
       "             ('s', 26),\n",
       "             ('part', 52),\n",
       "             ('amazing', 26),\n",
       "             ('open', 26),\n",
       "             ('effort', 26),\n",
       "             ('unprecedented', 26),\n",
       "             ('level', 26),\n",
       "             ('acquisition', 26),\n",
       "             ('sharing', 26),\n",
       "             ('among', 26),\n",
       "             ('researchers', 26),\n",
       "             ('scientific', 26),\n",
       "             ('statements', 52),\n",
       "             ('judgments', 52),\n",
       "             ('like', 52),\n",
       "             ('these', 104),\n",
       "             ('those', 52),\n",
       "             ('professionals', 52),\n",
       "             ('knowledgeable', 26),\n",
       "             ('really', 26),\n",
       "             ('helpful', 26),\n",
       "             ('especially', 52),\n",
       "             ('our', 234),\n",
       "             ('government', 26),\n",
       "             ('various', 52),\n",
       "             ('not', 52),\n",
       "             ('only', 26),\n",
       "             ('deeper', 26),\n",
       "             ('understanding', 26),\n",
       "             ('happening', 26),\n",
       "             ('but', 52),\n",
       "             ('become', 26),\n",
       "             ('basis', 26),\n",
       "             ('going', 26),\n",
       "             ('respond', 26),\n",
       "             ('handle', 26),\n",
       "             ('things', 26),\n",
       "             ('crucial', 26),\n",
       "             ('helping', 52),\n",
       "             ('find', 26),\n",
       "             ('probable', 32),\n",
       "             ('or', 52),\n",
       "             ('possible', 35),\n",
       "             ('cure', 31),\n",
       "             ('solution', 26),\n",
       "             ('end', 28),\n",
       "             ('pandemic', 78),\n",
       "             ('get', 78),\n",
       "             ('back', 26),\n",
       "             ('normal', 26),\n",
       "             ('lives', 78),\n",
       "             ('where', 26),\n",
       "             ('enjoy', 26),\n",
       "             ('company', 26),\n",
       "             ('family', 26),\n",
       "             ('friends', 26),\n",
       "             ('once', 26),\n",
       "             ('more', 52),\n",
       "             ('conclusion', 26),\n",
       "             ('just', 26),\n",
       "             ('branches', 26),\n",
       "             ('deals', 26),\n",
       "             ('life', 26),\n",
       "             ('living', 26),\n",
       "             ('organisms', 26),\n",
       "             ('teacher', 26),\n",
       "             ('taught', 26),\n",
       "             ('school', 26),\n",
       "             ('actually', 26),\n",
       "             ('daily', 26),\n",
       "             ('helped', 26),\n",
       "             ('own', 26),\n",
       "             ('bodies', 26),\n",
       "             ('each', 52),\n",
       "             ('day', 52),\n",
       "             ('well', 26),\n",
       "             ('sciences', 26),\n",
       "             ('technology', 26),\n",
       "             ('everything', 26),\n",
       "             ('thus', 26),\n",
       "             ('making', 26),\n",
       "             ('survive', 26),\n",
       "             ('now', 26),\n",
       "             ('experiencing', 26),\n",
       "             ('global', 26),\n",
       "             ('caused', 26),\n",
       "             ('called', 26),\n",
       "             ('i', 26),\n",
       "             ('hope', 26),\n",
       "             ('everyone', 26),\n",
       "             ('see', 26),\n",
       "             ('difference', 26),\n",
       "             ('fields', 26),\n",
       "             ('make', 26),\n",
       "             ('order', 26),\n",
       "             ('let', 26),\n",
       "             ('support', 26),\n",
       "             ('salute', 26),\n",
       "             ('brave', 26),\n",
       "             ('frontliners', 26),\n",
       "             ('despite', 26),\n",
       "             ('risks', 25),\n",
       "             ('exposure', 23),\n",
       "             ('still', 19),\n",
       "             ('continue', 18),\n",
       "             ('do', 16),\n",
       "             ('their', 15),\n",
       "             ('best', 24),\n",
       "             ('finding', 12),\n",
       "             ('care', 8)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Numpy Matrix\n",
    "import numpy as np\n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dense(150, activation='relu'))\n",
    "\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "   \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(589, 25)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocabulary_size+1)\n",
    "seq_len = X.shape[1]\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 25)            7050      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 25, 150)           105600    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 150)               180600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 150)               22650     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 282)               42582     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 358,482\n",
      "Trainable params: 358,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = create_model(vocabulary_size+1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "5/5 [==============================] - 3s 101ms/step - loss: 5.6407 - accuracy: 0.0374\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 5.6224 - accuracy: 0.0560\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 1s 127ms/step - loss: 5.4612 - accuracy: 0.0560\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 1s 130ms/step - loss: 5.2725 - accuracy: 0.0458\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 5.1975 - accuracy: 0.0509\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 5.1616 - accuracy: 0.0407\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 5.1413 - accuracy: 0.0560\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 5.1291 - accuracy: 0.0560\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 5.1215 - accuracy: 0.0509\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 5.1191 - accuracy: 0.0543\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 5.1166 - accuracy: 0.0560\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 5.1161 - accuracy: 0.0560\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 5.1162 - accuracy: 0.0560\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 5.1130 - accuracy: 0.0560\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 5.1153 - accuracy: 0.0560\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 5.1141 - accuracy: 0.0560\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 5.1127 - accuracy: 0.0509\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 5.1106 - accuracy: 0.0509\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 5.1112 - accuracy: 0.0560\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 5.1123 - accuracy: 0.0560\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 5.1110 - accuracy: 0.0560\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 5.1100 - accuracy: 0.0560\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 5.1127 - accuracy: 0.0560\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 5.1131 - accuracy: 0.0560\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 5.1107 - accuracy: 0.0560\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 5.1108 - accuracy: 0.0509\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 5.1111 - accuracy: 0.0577\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 5.1108 - accuracy: 0.0560\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 5.1103 - accuracy: 0.0560\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 5.1092 - accuracy: 0.0560\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 5.1092 - accuracy: 0.0509\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 5.1088 - accuracy: 0.0543\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 5.1086 - accuracy: 0.0560\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 5.1089 - accuracy: 0.0560\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 5.1073 - accuracy: 0.0560\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 5.1057 - accuracy: 0.0560\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 5.1042 - accuracy: 0.0560\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 5.1016 - accuracy: 0.0560\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 5.0989 - accuracy: 0.0560\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 5.0945 - accuracy: 0.0560\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 5.0901 - accuracy: 0.0560\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 5.0775 - accuracy: 0.0560\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 5.0617 - accuracy: 0.0628\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 5.0390 - accuracy: 0.0662\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 5.0080 - accuracy: 0.0611\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 4.9874 - accuracy: 0.0713\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 4.9551 - accuracy: 0.0594\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 4.8886 - accuracy: 0.0696\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 4.8390 - accuracy: 0.0679\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 4.7783 - accuracy: 0.0611\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 4.7099 - accuracy: 0.0696\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 4.6669 - accuracy: 0.0747\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 4.5750 - accuracy: 0.0730\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 4.5456 - accuracy: 0.0713\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 4.4531 - accuracy: 0.0764\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 4.3724 - accuracy: 0.0849\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 4.2879 - accuracy: 0.0883\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 4.2219 - accuracy: 0.0917\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 4.1525 - accuracy: 0.0968\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 4.0919 - accuracy: 0.1036\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 4.0507 - accuracy: 0.1053\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 3.9509 - accuracy: 0.1104\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 3.8952 - accuracy: 0.1222\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 3.8088 - accuracy: 0.1426\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 3.7426 - accuracy: 0.1205\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 3.6862 - accuracy: 0.1460\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 3.6147 - accuracy: 0.1494\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 3.5734 - accuracy: 0.1460\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 3.4835 - accuracy: 0.1545\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 3.3981 - accuracy: 0.1766\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 3.3524 - accuracy: 0.1681\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 3.2893 - accuracy: 0.1613\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 3.2044 - accuracy: 0.1817\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 3.1504 - accuracy: 0.1868\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 3.1065 - accuracy: 0.2122\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 3.0328 - accuracy: 0.2139\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 2.9723 - accuracy: 0.2377\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 2.9229 - accuracy: 0.2343\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 2.9039 - accuracy: 0.2360\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 2.8195 - accuracy: 0.2632\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 2.7589 - accuracy: 0.2733\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 2.7104 - accuracy: 0.2784\n",
      "Epoch 83/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 156ms/step - loss: 2.6816 - accuracy: 0.2869\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 2.6362 - accuracy: 0.2920\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 2.5481 - accuracy: 0.3124\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 2.5134 - accuracy: 0.3158\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 2.4298 - accuracy: 0.3548\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 2.3731 - accuracy: 0.3905\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 2.2994 - accuracy: 0.4143\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 2.2498 - accuracy: 0.4278\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 2.2100 - accuracy: 0.4414\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 2.1753 - accuracy: 0.4295\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 2.1186 - accuracy: 0.4601\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 2.0759 - accuracy: 0.4635\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 2.0410 - accuracy: 0.4924\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 1.9711 - accuracy: 0.5161\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 1.9549 - accuracy: 0.5246\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 1.9307 - accuracy: 0.5467\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 1.8816 - accuracy: 0.5603\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 1.8470 - accuracy: 0.5518\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 1.8043 - accuracy: 0.5739\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 1.7479 - accuracy: 0.5772\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 1.7044 - accuracy: 0.6027\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 1.6637 - accuracy: 0.6078\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 1.6484 - accuracy: 0.6316\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 1.5682 - accuracy: 0.6418\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 1.5309 - accuracy: 0.6604\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 1.5052 - accuracy: 0.6553\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 1.4704 - accuracy: 0.6638\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 1.4647 - accuracy: 0.6706\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 1.3832 - accuracy: 0.6927\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 1.3469 - accuracy: 0.7080\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 1.3120 - accuracy: 0.7097\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 1.2729 - accuracy: 0.7368\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 1.2350 - accuracy: 0.7487\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 1.1797 - accuracy: 0.7589\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 1.1469 - accuracy: 0.7810\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 1.1242 - accuracy: 0.7759\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 1.0874 - accuracy: 0.7759\n",
      "Epoch 120/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 1.0476 - accuracy: 0.7912\n",
      "Epoch 121/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 1.0154 - accuracy: 0.8065\n",
      "Epoch 122/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.9836 - accuracy: 0.8081\n",
      "Epoch 123/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.9668 - accuracy: 0.8149\n",
      "Epoch 124/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.9365 - accuracy: 0.8115\n",
      "Epoch 125/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.9040 - accuracy: 0.8302\n",
      "Epoch 126/300\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.8867 - accuracy: 0.8268\n",
      "Epoch 127/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.8501 - accuracy: 0.8540\n",
      "Epoch 128/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.8184 - accuracy: 0.8744\n",
      "Epoch 129/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.7799 - accuracy: 0.8659\n",
      "Epoch 130/300\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.7433 - accuracy: 0.8761\n",
      "Epoch 131/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.7216 - accuracy: 0.8879\n",
      "Epoch 132/300\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.6993 - accuracy: 0.8981\n",
      "Epoch 133/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.6665 - accuracy: 0.9083\n",
      "Epoch 134/300\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.6473 - accuracy: 0.8998\n",
      "Epoch 135/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.6279 - accuracy: 0.9049\n",
      "Epoch 136/300\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.6236 - accuracy: 0.8998\n",
      "Epoch 137/300\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.6052 - accuracy: 0.9151\n",
      "Epoch 138/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.5863 - accuracy: 0.9151\n",
      "Epoch 139/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.5673 - accuracy: 0.9202\n",
      "Epoch 140/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.5400 - accuracy: 0.9406\n",
      "Epoch 141/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.5094 - accuracy: 0.9372\n",
      "Epoch 142/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.4807 - accuracy: 0.9355\n",
      "Epoch 143/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.4618 - accuracy: 0.9423\n",
      "Epoch 144/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.4384 - accuracy: 0.9559\n",
      "Epoch 145/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.4220 - accuracy: 0.9525\n",
      "Epoch 146/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.4078 - accuracy: 0.9559\n",
      "Epoch 147/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.3849 - accuracy: 0.9626\n",
      "Epoch 148/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.3820 - accuracy: 0.9610\n",
      "Epoch 149/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.3628 - accuracy: 0.9643\n",
      "Epoch 150/300\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.3476 - accuracy: 0.9711\n",
      "Epoch 151/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3359 - accuracy: 0.9745\n",
      "Epoch 152/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3222 - accuracy: 0.9796\n",
      "Epoch 153/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3091 - accuracy: 0.9762\n",
      "Epoch 154/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.2942 - accuracy: 0.9762\n",
      "Epoch 155/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.2861 - accuracy: 0.9779\n",
      "Epoch 156/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.2715 - accuracy: 0.9830\n",
      "Epoch 157/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.2625 - accuracy: 0.9813\n",
      "Epoch 158/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.2526 - accuracy: 0.9813\n",
      "Epoch 159/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.2403 - accuracy: 0.9847\n",
      "Epoch 160/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.2357 - accuracy: 0.9847\n",
      "Epoch 161/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.2258 - accuracy: 0.9847\n",
      "Epoch 162/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.2149 - accuracy: 0.9898\n",
      "Epoch 163/300\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.2082 - accuracy: 0.9932\n",
      "Epoch 164/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 151ms/step - loss: 0.2039 - accuracy: 0.9898\n",
      "Epoch 165/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.1919 - accuracy: 0.9966\n",
      "Epoch 166/300\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.1821 - accuracy: 0.9949\n",
      "Epoch 167/300\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.1748 - accuracy: 0.9932\n",
      "Epoch 168/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.1659 - accuracy: 0.9949\n",
      "Epoch 169/300\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.1613 - accuracy: 0.9949\n",
      "Epoch 170/300\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.1570 - accuracy: 0.9966\n",
      "Epoch 171/300\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.1536 - accuracy: 0.9983\n",
      "Epoch 172/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.1485 - accuracy: 0.9966\n",
      "Epoch 173/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.1433 - accuracy: 1.0000\n",
      "Epoch 174/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.1352 - accuracy: 1.0000\n",
      "Epoch 175/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.1314 - accuracy: 1.0000\n",
      "Epoch 176/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.1271 - accuracy: 0.9966\n",
      "Epoch 177/300\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.1214 - accuracy: 0.9983\n",
      "Epoch 178/300\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.1164 - accuracy: 1.0000\n",
      "Epoch 179/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.1125 - accuracy: 1.0000\n",
      "Epoch 180/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.1091 - accuracy: 1.0000\n",
      "Epoch 181/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.1047 - accuracy: 1.0000\n",
      "Epoch 182/300\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.1050 - accuracy: 1.0000\n",
      "Epoch 183/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.1038 - accuracy: 1.0000\n",
      "Epoch 184/300\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.0982 - accuracy: 1.0000\n",
      "Epoch 185/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.0953 - accuracy: 1.0000\n",
      "Epoch 186/300\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.0925 - accuracy: 1.0000\n",
      "Epoch 187/300\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.0884 - accuracy: 1.0000\n",
      "Epoch 188/300\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.0849 - accuracy: 1.0000\n",
      "Epoch 189/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.0815 - accuracy: 1.0000\n",
      "Epoch 190/300\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.0781 - accuracy: 1.0000\n",
      "Epoch 191/300\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.0766 - accuracy: 1.0000\n",
      "Epoch 192/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.0731 - accuracy: 1.0000\n",
      "Epoch 193/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.0711 - accuracy: 1.0000\n",
      "Epoch 194/300\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.0695 - accuracy: 1.0000\n",
      "Epoch 195/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.0674 - accuracy: 1.0000\n",
      "Epoch 196/300\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.0650 - accuracy: 1.0000\n",
      "Epoch 197/300\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.0642 - accuracy: 1.0000\n",
      "Epoch 198/300\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.0616 - accuracy: 1.0000\n",
      "Epoch 199/300\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.0601 - accuracy: 1.0000\n",
      "Epoch 200/300\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.0583 - accuracy: 1.0000\n",
      "Epoch 201/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.0575 - accuracy: 1.0000\n",
      "Epoch 202/300\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.0555 - accuracy: 1.0000\n",
      "Epoch 203/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.0535 - accuracy: 1.0000\n",
      "Epoch 204/300\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.0521 - accuracy: 1.0000\n",
      "Epoch 205/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.0509 - accuracy: 1.0000\n",
      "Epoch 206/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.0491 - accuracy: 1.0000\n",
      "Epoch 207/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.0480 - accuracy: 1.0000\n",
      "Epoch 208/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.0466 - accuracy: 1.0000\n",
      "Epoch 209/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.0455 - accuracy: 1.0000\n",
      "Epoch 210/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.0443 - accuracy: 1.0000\n",
      "Epoch 211/300\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.0436 - accuracy: 1.0000\n",
      "Epoch 212/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.0427 - accuracy: 1.0000\n",
      "Epoch 213/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.0421 - accuracy: 1.0000\n",
      "Epoch 214/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.0410 - accuracy: 1.0000\n",
      "Epoch 215/300\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.0400 - accuracy: 1.0000\n",
      "Epoch 216/300\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.0389 - accuracy: 1.0000\n",
      "Epoch 217/300\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.0386 - accuracy: 1.0000\n",
      "Epoch 218/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.0376 - accuracy: 1.0000\n",
      "Epoch 219/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.0364 - accuracy: 1.0000\n",
      "Epoch 220/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.0356 - accuracy: 1.0000\n",
      "Epoch 221/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.0344 - accuracy: 1.0000\n",
      "Epoch 222/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.0338 - accuracy: 1.0000\n",
      "Epoch 223/300\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.0329 - accuracy: 1.0000\n",
      "Epoch 224/300\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.0322 - accuracy: 1.0000\n",
      "Epoch 225/300\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.0315 - accuracy: 1.0000\n",
      "Epoch 226/300\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.0309 - accuracy: 1.0000\n",
      "Epoch 227/300\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.0302 - accuracy: 1.0000\n",
      "Epoch 228/300\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.0294 - accuracy: 1.0000\n",
      "Epoch 229/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.0289 - accuracy: 1.0000\n",
      "Epoch 230/300\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.0283 - accuracy: 1.0000\n",
      "Epoch 231/300\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.0278 - accuracy: 1.0000\n",
      "Epoch 232/300\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.0274 - accuracy: 1.0000\n",
      "Epoch 233/300\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.0267 - accuracy: 1.0000\n",
      "Epoch 234/300\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.0260 - accuracy: 1.0000\n",
      "Epoch 235/300\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 236/300\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.0251 - accuracy: 1.0000\n",
      "Epoch 237/300\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.0244 - accuracy: 1.0000\n",
      "Epoch 238/300\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 239/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.0236 - accuracy: 1.0000\n",
      "Epoch 240/300\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 241/300\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.0228 - accuracy: 1.0000\n",
      "Epoch 242/300\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.0224 - accuracy: 1.0000\n",
      "Epoch 243/300\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 244/300\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 245/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 145ms/step - loss: 0.0212 - accuracy: 1.0000\n",
      "Epoch 246/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 247/300\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.0206 - accuracy: 1.0000\n",
      "Epoch 248/300\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.0202 - accuracy: 1.0000\n",
      "Epoch 249/300\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.0198 - accuracy: 1.0000\n",
      "Epoch 250/300\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 251/300\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 252/300\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 253/300\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 254/300\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 255/300\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.0178 - accuracy: 1.0000\n",
      "Epoch 256/300\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.0176 - accuracy: 1.0000\n",
      "Epoch 257/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 258/300\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 259/300\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 260/300\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 261/300\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 262/300\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 263/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 264/300\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 265/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 266/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 267/300\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 268/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 269/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 270/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 271/300\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 272/300\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 273/300\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 274/300\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 275/300\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 276/300\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 277/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 278/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 279/300\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 280/300\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 281/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 282/300\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 283/300\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 284/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 285/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 286/300\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 287/300\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 288/300\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 289/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 290/300\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 291/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 292/300\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 293/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 294/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 295/300\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 296/300\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 297/300\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 298/300\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 299/300\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 300/300\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.0092 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bbffd967f0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=300,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('epochBIG.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('epochBIG', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    model : model that was trained on text data\n",
    "    tokenizer : tokenizer that was fit on text data\n",
    "    seq_len : length of training sequence\n",
    "    seed_text : raw string text to serve as the seed\n",
    "    num_gen_words : number of words to be generated by model\n",
    "    '''\n",
    "    \n",
    "    # Final Output\n",
    "    output_text = []\n",
    "    \n",
    "    # Intial Seed Sequence\n",
    "    input_text = seed_text\n",
    "    \n",
    "    # Create num_gen_words\n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "        # Take the input text string and encode it to a sequence\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "        # Pad sequences to our trained rate\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "        \n",
    "        # Predict probabilities for each word\n",
    "        pred_probabilities = model.predict(pad_encoded, verbose=0)[0]\n",
    "        \n",
    "        # Find the index of the word with the highest probability\n",
    "        pred_word_ind = np.argmax(pred_probabilities)\n",
    "        \n",
    "        # Grab word\n",
    "        pred_word = tokenizer.index_word[pred_word_ind] \n",
    "        \n",
    "        # Update the sequence of input text (shifting one over with the new word)\n",
    "        input_text += ' ' + pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "        \n",
    "    # Make it look like a sentence.\n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'new',\n",
       " 'virus',\n",
       " 'she',\n",
       " 'stated',\n",
       " 'that',\n",
       " 'the',\n",
       " 'virus',\n",
       " 'has',\n",
       " 'rna',\n",
       " 'in',\n",
       " 'its',\n",
       " 'genome',\n",
       " 'and',\n",
       " 'it',\n",
       " 'will',\n",
       " 'help',\n",
       " 'them',\n",
       " 'understand',\n",
       " 'how',\n",
       " 'scientists',\n",
       " 'and',\n",
       " 'medical',\n",
       " 'practitioners',\n",
       " 'will',\n",
       " 'test']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random_pick = random.randint(0,len(text_sequences))\n",
    "random_seed_text = text_sequences[random_pick]\n",
    "random_seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the new virus she stated that the virus has rna in its genome and it will help them understand how scientists and medical practitioners will test'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text = ' '.join(random_seed_text)\n",
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for the presence of coronavirus they were also able to take a look at the changes in the sequence of the viral genome which is a big help in tracking the spread of the virus around the world matt daugherty another professor who studies the evolutionary arms race that pits'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
